{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Models_FeatureSelection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePp_nTb2dbEE"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNXBecwpdjAD"
      },
      "source": [
        "!pip install scipy==1.1.0\n",
        "!pip install pandas==0.23.4\n",
        "!pip install numpy==1.15.2\n",
        "!pip install seaborn==0.9.0\n",
        "!pip install matplotlib==3.0.0\n",
        "!pip install imbalanced_learn==0.4.3\n",
        "!pip install xgboost==0.90\n",
        "!pip install lightgbm==2.2.3\n",
        "!pip install imblearn==0.0\n",
        "!pip install scikit_learn==0.21.3\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fg88Y0Xsd4He"
      },
      "source": [
        "!pip install numpy --upgrade\n",
        "!pip install --upgrade pandas\n",
        "!pip install lightgbm --upgrade\n",
        "!pip install -U scikit-learn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HySkiKa8W2jt"
      },
      "source": [
        "!pip install six\n",
        "!pip install django_taggit==1.2.0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbBdlCrUeFw6"
      },
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_recall_curve, confusion_matrix, classification_report, roc_curve, auc, f1_score, recall_score, accuracy_score, mean_squared_error\n",
        "from sklearn.model_selection import cross_val_score, RandomizedSearchCV, train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import statistics\n",
        "import six\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import xgboost as xgb\n",
        "from scipy.stats import uniform, randint\n",
        "from scipy import interp\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eDLLVC5tUXT"
      },
      "source": [
        "!pip install feature_selector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--jjBXEXevVe"
      },
      "source": [
        "pip install --upgrade pip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxXU6l2TtXmB"
      },
      "source": [
        "from feature_selector import FeatureSelector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHdLTyzEi0rk"
      },
      "source": [
        "!pip install --upgrade pandas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8yaC-F_iHb5"
      },
      "source": [
        "!pip install numpy --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2-wWlzkjBnC"
      },
      "source": [
        "!pip install lightgbm --upgrade\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_uSNC4pgYjV"
      },
      "source": [
        "#################\n",
        "### Arguments ###\n",
        "#################\n",
        "\n",
        "parser = argparse.ArgumentParser(description='EGFR and KRAS Lung Cancer Mutation Status - Train')\n",
        "parser.add_argument('--gene', type=str,\n",
        "                    default='EGFR',\n",
        "                    help='pick gene - EGFR / KRAS (default: EGFR)')\n",
        "parser.add_argument('--path-EGFR', type=str,\n",
        "                    default=r\"/content/drive/MyDrive/Colab_Notebooks/lucasradsemegfrkras-master/Radiomics/data/EGFR/FEATURES.xlsx\",\n",
        "                    help='data path for EGFR (default:/content/drive/MyDrive/Colab_Notebooks/lucasradsemegfrkras-master/Radiomics/data/EGFR/FEATURES.xlsx)' )\n",
        "parser.add_argument('--path-KRAS', type=str,\n",
        "                    default=r\"/content/drive/MyDrive/Colab_Notebooks/lucasradsemegfrkras-master/Radiomics/data/EGFR/FEATURES.xlsx)'\",\n",
        "                    help='data path for KRAS (default: /content/drive/MyDrive/Colab_Notebooks/lucasradsemegfrkras-master/Radiomics/data/EGFR/FEATURES.xlsx)')\n",
        "parser.add_argument('--n-tests', type=int,\n",
        "                    default=50,\n",
        "                    help='number of tests (default: 100)')\n",
        "args = parser.parse_args(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PumfdXhGgbXM"
      },
      "source": [
        "#################\n",
        "### Functions ###\n",
        "#################\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, classes,\n",
        "                          normalize=False,\n",
        "                          title=None,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion Matrix'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Only use the labels that appear in the data\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion Matrix')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_yticklabels(), rotation=90, va=\"center\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    plt.savefig('confusionmatrix.jpg')\n",
        "    return ax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RwPL0hLgb3j"
      },
      "source": [
        "############\n",
        "### Data ###\n",
        "############\n",
        "\n",
        "if args.gene=='EGFR':\n",
        "    # Path of the features regarding EGFR mutation status\n",
        "    features = pd.read_excel(args.path_EGFR)\n",
        "\n",
        "    labels = np.array(features['EGFR mutation status'])\n",
        "    ids=np.array(features['Case ID'])\n",
        "\n",
        "    # Remove labels from the features\n",
        "    features= features.drop(['EGFR mutation status', 'Case ID'], axis = 1)\n",
        "elif args.gene=='KRAS':\n",
        "    # Path of the features regarding KRAS mutation status\n",
        "    features = pd.read_excel(args.path_KRAS)\n",
        "\n",
        "    # Labels are the values we want to predict\n",
        "    labels = np.array(features['KRAS mutation status'])\n",
        "    ids=np.array(features['Case ID'])\n",
        "\n",
        "    # Remove labels from the features\n",
        "    features= features.drop(['KRAS mutation status', 'Case ID'], axis = 1)\n",
        "    # tira as duas colunas q não são features\n",
        "else:\n",
        "    print(\"Error: Invalid gene!\")\n",
        "\n",
        "#print(features)\n",
        "\n",
        "# One-hot encode categorical features\n",
        "features = pd.get_dummies(features, prefix=['Gender', 'Smoking status'])\n",
        "# Binariza as features de género e smoking status para 4: M, F, S, NS\n",
        "\n",
        "print('The shape of our features is:', features.shape)\n",
        "\n",
        "# Saving feature names for later use\n",
        "feature_list = list(features.columns)\n",
        "# guarda o nome de todas as features numa lista\n",
        "\n",
        "# Convert to numpy array\n",
        "features = np.array(features)\n",
        "print(features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfhAv02BghnE"
      },
      "source": [
        "################\n",
        "### Training ###\n",
        "################\n",
        "FPRXBTotal=[]\n",
        "AucXBTotal=[]\n",
        "F1XBTotal=[]\n",
        "SensitivityXBTotal=[]\n",
        "PrecisionXBTotal=[]\n",
        "SpecificitynXBTotal=[]\n",
        "tprs = []\n",
        "aucs = []\n",
        "mean_fpr = np.linspace(0, 1, 500) # cria um np array com 500 nºs entre 0 e 1\n",
        "\n",
        "# Make n_tests splits, training xgboost with Cross Validation hyperparameter tuning for each one\n",
        "for i in range(0, args.n_tests):\n",
        "\n",
        "    print('######## Test number {} ########\\n'.format(i+1))\n",
        "\n",
        "    # Split the data into training set and test set (80/20% division)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, shuffle=True, stratify=labels)\n",
        "    # função do scikit_learn.model_selection\n",
        "\n",
        "    # Normalization - only radiomic features are normlaized, clinical are not considered\n",
        "    cols_to_norm = np.arange(0,features.shape[1]-5)\n",
        "    scaler = MinMaxScaler()\n",
        "    X_train[:,cols_to_norm] = scaler.fit_transform(X_train[:,cols_to_norm])\n",
        "    X_test[:,cols_to_norm] = scaler.transform(X_test[:,cols_to_norm])\n",
        "\n",
        "    # Create pandas data frame\n",
        "    X_train=pd.DataFrame(X_train, columns=feature_list)\n",
        "    y_train=pd.DataFrame(y_train)\n",
        "    X_test=pd.DataFrame(X_test, columns=feature_list)\n",
        "    y_test=pd.DataFrame(y_test)\n",
        "\n",
        "    #X_train = np.array(X_train)\n",
        "    #Q, R= np.linalg.qr(X_train)\n",
        "    #min_tol = 1e-9\n",
        "    #indep = np.where(np.abs(R.diagonal()) >  min_tol)[0]\n",
        "    #print(indep)\n",
        "    #print(X_train[:, indep])\n",
        "\n",
        "    #X_train = np.array(X_train)\n",
        "   # X_test=np.array(X_test)\n",
        "   # Q, R= np.linalg.qr(X_train)\n",
        "    #min_tol = 1e-9\n",
        "   # indep = np.where(np.abs(R.diagonal()) >  min_tol)[0]\n",
        "   # X_train= X_train[:, indep]\n",
        "    #Q1, R1= np.linalg.qr(X_test)\n",
        "    #indep = np.where(np.abs(R1.diagonal()) >  min_tol)[0]\n",
        "   # X_test= X_test[:, indep]\n",
        "\n",
        "    #########PCA######################\n",
        "    pca_cancer = PCA(n_components=0.75)\n",
        "    X_train = pca_cancer.fit_transform(X_train)\n",
        "    X_test = pca_cancer.transform(X_test)\n",
        "\n",
        "    # Create Feature Selector instance\n",
        "    #fs = FeatureSelector(data = X_train, labels = y_train)\n",
        "\n",
        "    #PAIRWISE CORRELATION FILTER!!!!!!!!!!!!!!!\n",
        "    # Remove higlhy correlated features\n",
        "    #fs.identify_collinear(correlation_threshold=0.95)\n",
        "    #correlated_features = fs.ops['collinear']\n",
        "    #train_no_collinear = fs.remove(methods = ['collinear'])\n",
        "    #test_no_collinear = X_test.drop(correlated_features, axis=1)\n",
        "    #collinear=len(correlated_features)\n",
        "    # Remove zero and low importance features\n",
        "    # NOTE: Change the hyper-parameters values of the GBM in the feature_selector.py file\n",
        "    # EGFR---> n_estimators:1000, learning rate: 0.02\n",
        "    # KRAS---> n_estimators:400, learning rate: 0.1\n",
        "\n",
        "    # Number of features after removing collinear features\n",
        "    #print('Number of features after removing collinear features:', train_no_collinear.shape)\n",
        "\n",
        "    #REMOVE ZERO IMPORTANCE FEATURES!!!!!!!!!!!!!!!\n",
        "    # Create new Feature Selector instance\n",
        "    #fs = FeatureSelector(data = train_no_collinear, labels = y_train)\n",
        "\n",
        "    # Identify zero importance features\n",
        "   # fs.identify_zero_importance(task = 'classification',\n",
        "         #                   eval_metric = 'auc',\n",
        "        # #                   n_iterations = 10,\n",
        "           #                  early_stopping = True)\n",
        "\n",
        "    #Identify low importance features\n",
        "    #fs.identify_low_importance(cumulative_importance = 0.95)\n",
        "    #zero_importance_features = fs.ops['zero_importance']\n",
        "    #zero=len(zero_importance_features)\n",
        "\n",
        "\n",
        "    #removed_features.append(collinear+zero)\n",
        "\n",
        "    #Remove features from training set\n",
        "    #final_training_set = fs.remove(methods = ['zero_importance'])\n",
        "\n",
        "    #Removing features from test set\n",
        "    #final_test_set = test_no_collinear.drop(zero_importance_features, axis=1)\n",
        "\n",
        "    #Number of features after removing zero and low importance features\n",
        "    #print('Number of features after removing zero and low importance features:', len(final_training_set.columns))\n",
        "\n",
        "    # Saving feature names for later use\n",
        "    #feature_list_final = list(final_training_set.columns)\n",
        "\n",
        "    # Convert to numpy array\n",
        "    X_train = np.array(X_train)\n",
        "    X_test = np.array(X_test)\n",
        "\n",
        "    print('Training Features Shape:', X_train.shape)\n",
        "    print('Testing Features Shape:', X_test.shape)\n",
        "\n",
        "    # Data augmentation using SMOTE\n",
        "    # NOTE: In both EGFR and KRAS cases none of the clinical features\n",
        "    # is preserved after feature selection. That's why SMOTE is used\n",
        "    # instead of SMOTE-NC\n",
        "    sm = SMOTE(random_state=42)\n",
        "    X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "    print('Number of samples after SMOTE:', X_train_res.shape)\n",
        "\n",
        "    X_train = np.array(X_train)\n",
        "    X_test=np.array(X_test)\n",
        "\n",
        "    # define SVN\n",
        "\n",
        "\n",
        "    #svclassifier = SVC(kernel='linear', probability=True)\n",
        "    #param_grid = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001]}\n",
        "    #search = GridSearchCV(svclassifier,param_grid,refit=True,verbose=2)\n",
        "    svclassifier = SVC(kernel='poly', probability=True)\n",
        "\n",
        "    param_grid = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001], 'degree':[2,3] }\n",
        "\n",
        "    # Intialize and fit randomized search\n",
        "    search = GridSearchCV(svclassifier,param_grid,refit=True,verbose=2)\n",
        "    #search.fit(X_train_res, y_train_res)\n",
        "    #clf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "    #params = {\n",
        "    #\"max_depth\": [5, 8, 15, 25, 30],\n",
        "    #\"n_estimators\": [100, 300, 500, 800, 1200],\n",
        "    #\"max_features\": [\"auto\", \"sqrt\"],\n",
        "    #\"criterion\": [\"gini\", \"entropy\"],\n",
        "    #\"bootstrap\": [True, False],\n",
        "    #\"min_samples_leaf\": [1, 2, 5, 10],\n",
        "    #\"min_samples_split\": [2, 5, 10, 15, 100],\n",
        "    #}\n",
        "    #search = RandomizedSearchCV(clf, param_distributions=params, n_iter=15, scoring='roc_auc', cv=5, verbose=3)\n",
        "    #search.fit(X_train_res, y_train_res)\n",
        "\n",
        "    #dual=[True,False]\n",
        "    #max_iter=[100,110,120,130,140]\n",
        "    #C = [0.01, 0.1, 1.0,1.5,2.0,2.5, 10, 100]\n",
        "    #param_grid = dict(dual=dual,max_iter=max_iter,C=C)\n",
        "\n",
        "    #lr = LogisticRegression(penalty='l2')\n",
        "    #search = GridSearchCV(estimator=lr, param_grid=param_grid, scoring = 'roc_auc', cv=5)\n",
        "        # define ElasticNet\n",
        "    #ENclassifier = ElasticNet()\n",
        "\n",
        "    #parametersGrid = {\"max_iter\": [1, 5, 10],\n",
        "        #              \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
        "        #              \"l1_ratio\": np.arange(0.0, 1.0, 0.1)}\n",
        "\n",
        "    #cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=42)\n",
        "\n",
        "    # Intialize and fit randomized search\n",
        "    #search = GridSearchCV(ENclassifier,parametersGrid,cv=cv, scoring='roc_auc')\n",
        "    #search.fit(X_train_res, y_train_res)\n",
        "\n",
        "    # Print best hyperparameters according to CV\n",
        "    #print(\"Best parameters set found on development set:\")\n",
        "    #print()\n",
        "    #print(search.best_params_)\n",
        "    #print()\n",
        "    #print(search.best_estimator_)\n",
        "\n",
        "     #define xgboost\n",
        "    #xgb_model = xgb.XGBClassifier(nthread=1, learning_rate=0.01, n_estimators=600, objective=\"binary:logistic\", random_state=42)\n",
        "\n",
        "    #params = {\n",
        "       #\"classification__colsample_bytree\": uniform(0.3, 0.7),\n",
        "      ## \"classification__gamma\": uniform(0.5, 2),\n",
        "       #\"classification__subsample\": uniform(0.6, 0.4),\n",
        "      # \"classification__learning_rate\": uniform(0.03, 0.3),\n",
        "      # \"classification__max_depth\": randint(2, 6),\n",
        "      # \"classification__n_estimators\": randint(100, 1000),\n",
        "      # 'classification__min_child_weight': randint(1, 5)\n",
        "  # }\n",
        "\n",
        "     #Intialize and fit randomized search\n",
        "    #search = RandomizedSearchCV(xgb_model, param_distributions=params, n_iter=15, scoring='roc_auc', n_jobs=1, cv=5, verbose=3)\n",
        "    search.fit(X_train_res, y_train_res)\n",
        "\n",
        "\n",
        "\n",
        "    # Test XGBOOST\n",
        "    y_pred=search.predict_proba(X_test)[:,1]\n",
        "\n",
        "\n",
        "    #ROC curve\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "    print('AUC:', auc(fpr, tpr))\n",
        "    AucXBTotal.append(auc(fpr, tpr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eml4LNZgVGPh"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XME6OMNhqyMi"
      },
      "source": [
        "print(\"Mean AUC for XB\", sum(AucXBTotal)/len(AucXBTotal), \"+\\-\", statistics.stdev(AucXBTotal))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHXN3PGsmoQT"
      },
      "source": [
        "F1XBTotal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69knvRoDrPSr"
      },
      "source": [
        "a=[0.3636363636363636,\n",
        " 0.5,\n",
        " 0.3076923076923077,\n",
        " 0.625,\n",
        " 0.3636363636363636,\n",
        " 0.20000000000000004,\n",
        " 0.6153846153846154,\n",
        " 0.14285714285714285,\n",
        " 0.3076923076923077,\n",
        " 0.4444444444444445,\n",
        " 0.6,\n",
        " 0.6,\n",
        " 0.4444444444444444,\n",
        " 0.5333333333333333,\n",
        " 0.28571428571428575,\n",
        " 0.3636363636363636,\n",
        " 0.3636363636363636,\n",
        " 0.3076923076923077,\n",
        " 0.5,\n",
        " 0.3636363636363636,\n",
        " 0.1818181818181818,\n",
        " 0.5714285714285714,\n",
        " 0.28571428571428575,\n",
        " 0.5454545454545454,\n",
        " 0.6,\n",
        " 0.42857142857142855,\n",
        " 0.5454545454545454,\n",
        "\n",
        " 0.5,\n",
        " 0.3333333333333333,\n",
        " 0.4444444444444445,\n",
        " 0.25,\n",
        " 0.5,\n",
        " 0.4615384615384615,\n",
        " 0.3636363636363636,\n",
        "\n",
        " 0.16666666666666666,\n",
        " 0.26666666666666666,\n",
        " 0.6,\n",
        "\n",
        " 0.4615384615384615,\n",
        " 0.5714285714285714,\n",
        " 0.20000000000000004,\n",
        " 0.25,\n",
        " 0.5714285714285714,\n",
        "\n",
        " 0.5454545454545454,\n",
        " 0.3076923076923077,\n",
        " 0.22222222222222224,\n",
        " 0.3333333333333333]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxUtQi0irUi8"
      },
      "source": [
        "sum(a)/len(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TX0lv_drXVU"
      },
      "source": [
        "statistics.stdev(a)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}